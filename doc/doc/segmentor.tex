\documentclass[12pt]{article}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{CJK}
\usepackage{times}
 \DeclareMathSizes{10}{24.88}{20.74}{17.28}
\makeindex
\title{Chinese Word Segmentation}
%   \author{Yue Zhang \hspace{1cm} Muhua Zhu}
\begin{document}

\begin{CJK}{GBK}{song}
\maketitle

\section{How to compile}
Suppose that Zpar has been downloaded to the directory \textit{zpar}. To make the segmenter system, type \textit{make segmentor}. This will create a directory \textit{zpar/dist/segmentor}, in which there are two files: \textit{train} and {\em segmenter}. The file \textit{train} is used to train a segmentation model,and the file \textit{segmentor} is used to segment new texts using a trained segmentation model.
\section{Format of inputs and outputs}
The input files to the segmentor are formatted as a sequence of Chinese characters. An example input is:
\\
\\
\hspace{3cm} ZPar可以分析中文和英文
\\
\\
The output files contain space-separated words:
\\
\\
\hspace{3cm} ZPar 可以 分析 中文 和 英文
\\
\\
The output format is also the format of training files for the \textit{train} file.
\\
Both input and output files must be encoded in \textit{utf8}. Here is a \href{seg_files/gb2utf.py}{script} that transfers files that are encoded in \textit{gb} to the \textit{utf8} encoding.
\section{How to train a model}
To train a model, use
\\
\\
\hspace{3cm} zpar/dist/segmentor/train $<$train-file$>$ $<$model-file$>$ $<$number of iterations$>$
\\
\\
For example, using the \href{seg_files/train.txt}{train file}, you can train a  model by
\\
\\
\hspace{3cm} zpar/dist/segmentor/train train.txt model 1
\\
\\
After training is completed, a new file \textit{model} will be created in the current directory, which can be used to segment Chinese. The above command performs training with one iteration using the training file.
\section{How to segment new texts}
To apply an existing model to segment new texts, use
\\
\\
\hspace{3cm} zpar/dist/segmentor/segmentor $<$model$>$ $[<$input-file$>]$ $[<$output-file$>]$
\\
\\
where the input file and output file are optional. If the output file is not specified, segmented texts will be printed to the console. If the input file is not specified, raw texts will be read from the console. For example, using the model we just trained, we can segment \href{seg_files/input.txt}{an example input} by
\\
\\
\hspace{3cm} zpar/dist/segmentor/segmentor model input.txt output.txt
\\
\\
The output file contains automatically segmented texts.
\section{Outputs and evaluation}
Automatically segmented texts contain errors. In order to evaluate the quality of the outputs, we can manually specify the segmentation of a sample, and compare the outputs with the correct sample.
\\
A manually specified segmentation of the input file is given in \href{seg_files/reference.txt}{reference file}. Here is a \href{seg_files/evaluate.py}{Python script} that performs automatic evaluation.
\\
Using the above \textit{output.txt} and \textit{reference.txt}, we can evaluate the accuracies by typing
\\
\\
\hspace{3cm} python evaluate.py output.txt reference.txt
\\
\\
You can find the precision, recall, and f-score here.
\section{How to tune the performance of a system}
The performance of the system after one training iteration may not be optimal. You can try training a model for another few iterations, after each you compare the performance. You can choose the model that gives the highest f-score on your test data. We conventionally call this test file the development test data, because you develop a segmentation model using this. Here is a \href{seg_files/test.sh}{a shell script} that automatically trains the segmentor for 30 iterations, and after the $i$th iteration, stores the model file to model.$i$. You can compare the f-score of all 30 iterations and choose model.$k$, which gives the best f-score, as the final model. In this file, this is a variable called \textit{segmentor}. You need to set this variable to the relative directory of \textit{zpar/dist/segmentor}.
\begin{thebibliography}{99}
\bibitem{bib-1}
%Yue Zhang and Stephen Clark. 2011. Syntactic Processing Using the Generalized Perceptron and Beam Search. \textit{Computational Linguistics}, 37(1):105-151.
Yue Zhang and Stephen Clark. 2007. Chinese Segmentation Using a Word-based Perceptron Algorithm. In {\em Proc. of ACL} 2007. pages 840-847.
%\bibitem{bib-2}
%Yue Zhang and Stephen Clark. 2011. Shift-reduce CCG parsing. In {\em Proc. of ACL 2011}, pages 683-692.
\end{thebibliography}
\end{CJK}
\end{document}
